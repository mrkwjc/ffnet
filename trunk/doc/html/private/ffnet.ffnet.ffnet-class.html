<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
          "DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <title>ffnet.ffnet.ffnet</title>
  <link rel="stylesheet" href="epydoc.css" type="text/css"></link>
</head>
<body bgcolor="white" text="black" link="blue" vlink="#204080"
      alink="#204080">

<!-- =========== START OF NAVBAR =========== -->
<table class="navbar" border="0" width="100%" cellpadding="0" bgcolor="#a0c0ff" cellspacing="0">
  <tr valign="center">
    <th class="navbar">&nbsp;&nbsp;&nbsp;<a class="navbar" href="ffnet-module.html">Home</a>&nbsp;&nbsp;&nbsp;</th>
    <th class="navbar">&nbsp;&nbsp;&nbsp;<a class="navbar" href="trees.html">Trees</a>&nbsp;&nbsp;&nbsp;</th>
    <th class="navbar">&nbsp;&nbsp;&nbsp;<a class="navbar" href="indices.html">Index</a>&nbsp;&nbsp;&nbsp;</th>
    <th class="navbar">&nbsp;&nbsp;&nbsp;<a class="navbar" href="help.html">Help</a>&nbsp;&nbsp;&nbsp;</th>
    <th class="navbar" width="100%"></th>
  </tr>
</table>
<table width="100%" cellpadding="0" cellspacing="0">
  <tr valign="top">
    <td width="100%">
      <font size="-1"><b class="breadcrumbs">
        <a href="ffnet-module.html">Package&nbsp;ffnet</a> ::
        <a href="ffnet.ffnet-module.html">Module&nbsp;ffnet</a> ::
        Class&nbsp;ffnet
      </b></font></br>
    </td>
    <td><table cellpadding="0" cellspacing="0">
      <tr><td align="right"><font size="-2">[show&nbsp;private&nbsp;|&nbsp;<a href="../public/ffnet.ffnet.ffnet-class.html">hide&nbsp;private</a>]</font></td></tr>
      <tr><td align="right"><font size="-2">[<a href="frames.html"target="_top">frames</a>&nbsp;|&nbsp;<a href="ffnet.ffnet.ffnet-class.html" target="_top">no&nbsp;frames</a>]</font></td></tr>
    </table></td>
</tr></table>

<!-- =========== START OF CLASS DESCRIPTION =========== -->
<h2 class="class">Class ffnet</h2>

<hr/>

<pre class="literalblock">
Feed-forward neural network main class.

NETWORK CREATION:
Creation of the network consist in delivering list of neuron 
connections:
    conec = [[1, 3], [2, 3], [0, 3] 
             [1, 4], [2, 4], [0, 4] 
             [3, 5], [4, 5], [0, 5]]
    net = ffnet(conec)
0 (zero) in conec is a special unit representing bias. If there is
no connection from 0, bias is not considered in the node.
Only feed-forward directed graphs are allowed. Class makes check
for cycles in the provided graph and raises TypeError if any.
All nodes (exept input ones) have sigmoid activation function.

Although generation of conec is left to the user, there are several
functions provided to facilitate this task. See description of 
the following functions: mlgraph, imlgraph, tmlgraph.
More architectures may be provided in the future.

Weights are automatically initialized at the network creation. They can
be reinitialized later with 'randomweights' method.

TRAINING NETWORK:
There are several training methods included, currently:
train_momentum, train_rprop, train_genetic, train_cg, 
train_bfgs, train_tnc.
The simplest usage is, for example:
    net.train_tnc(input, target)
where 'input' and 'target' is data to be learned. Class performs data
normalization by itself and records encoding/decoding information 
to be used during network recalling.
Class makes basic checks of consistency of data.

For information about training prameters see appropriate 
method description.

RECALLING NETWORK:
Usage of the trained network is as simple as function call:
    ans = net(inp)
or, alternatively:
    ans = net.call(inp)
where 'inp' - list of network inputs and 'ans' - list of network outputs
There is also possibility to retrieve partial derivatives of 
output vs. input at given input point:
    deriv = n.derivative(inp)
'deriv' is an array of the form:
    | o1/i1, o1/i2, ..., o1/in |
    | o2/i1, o2/i2, ..., o2/in |
    | ...                      |
    | om/i1, om/i2, ..., om/in |

LOADING/SAVING/EXPORTING NETWORK
There are three helper functions provided with this class:
savenet, loadnet and exportnet.
Basic usage:
    savenet(net, filename)   --&gt; pickles network
    net = loadnet(filename)  --&gt; loads pickled network
    exportnet(net, filename) --&gt; exports network to fortran source

PLOTS
If you have matplotlib installed, the network architecture can be 
drawn with:
    from ffnet.tools import drawffnet
    import pylab
    drawffnet(net)
    pylab.show()
This is a very basic solution, see drawffnet description for
limitations.
</pre>
<hr/>


<!-- =========== START OF METHOD SUMMARY =========== -->
<table class="summary" border="1" cellpadding="3" cellspacing="0" width="100%" bgcolor="white">
<tr bgcolor="#70b0f0" class="summary">
  <th colspan="2">Method Summary</th></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><a name="__init__"></a><span class="summary-sig"><span class="summary-sig-name">__init__</span>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>conec</span>)</span></code>
</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><a name="__call__"></a><span class="summary-sig"><span class="summary-sig-name">__call__</span>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>inp</span>)</span></code>
</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><a name="__repr__"></a><span class="summary-sig"><span class="summary-sig-name">__repr__</span>(<span class=summary-sig-arg>self</span>)</span></code>
</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="ffnet.ffnet.ffnet-class.html#call" class="summary-sig-name"><code>call</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>inp</span>)</span></code>
<br />
Returns network answer to input sequence</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="ffnet.ffnet.ffnet-class.html#derivative" class="summary-sig-name"><code>derivative</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>inp</span>)</span></code>
<br />
Returns partial derivatives of the network's 
output vs its input at given input point
in the following array:
    | o1/i1, o1/i2, ..., o1/in |
    | o2/i1, o2/i2, ..., o2/in |
    | ...</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="ffnet.ffnet.ffnet-class.html#randomweights" class="summary-sig-name"><code>randomweights</code></a>(<span class=summary-sig-arg>self</span>)</span></code>
<br />
Randomize weights due to Bottou proposition</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="ffnet.ffnet.ffnet-class.html#sqerror" class="summary-sig-name"><code>sqerror</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>input</span>,
          <span class=summary-sig-arg>target</span>)</span></code>
<br />
Returns 0.5*(sum of squared errors at output) for input and target 
arrays being first normalized.</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="ffnet.ffnet.ffnet-class.html#sqgrad" class="summary-sig-name"><code>sqgrad</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>input</span>,
          <span class=summary-sig-arg>target</span>)</span></code>
<br />
Returns gradient of sqerror vs.</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="ffnet.ffnet.ffnet-class.html#test" class="summary-sig-name"><code>test</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>input</span>,
          <span class=summary-sig-arg>target</span>,
          <span class=summary-sig-arg>iprint</span>,
          <span class=summary-sig-arg>filename</span>)</span></code>
<br />
Calculates output and parameters of regression line of targets vs.</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="ffnet.ffnet.ffnet-class.html#train_bfgs" class="summary-sig-name"><code>train_bfgs</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>input</span>,
          <span class=summary-sig-arg>target</span>,
          <span class="summary-sig-kwarg">**kwargs</span>)</span></code>
<br />
Train network with constrained version of quasi-Newton method
of Broyden, Fletcher, Goldfarb, and Shanno (BFGS).</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="ffnet.ffnet.ffnet-class.html#train_cg" class="summary-sig-name"><code>train_cg</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>input</span>,
          <span class=summary-sig-arg>target</span>,
          <span class="summary-sig-kwarg">**kwargs</span>)</span></code>
<br />
Train network with conjugate gradient algorithm using the
nonlinear conjugate gradient algorithm of Polak and Ribiere.</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="ffnet.ffnet.ffnet-class.html#train_genetic" class="summary-sig-name"><code>train_genetic</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>input</span>,
          <span class=summary-sig-arg>target</span>,
          <span class="summary-sig-kwarg">**kwargs</span>)</span></code>
<br />
Global weights optimization with genetic algorithm.</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="ffnet.ffnet.ffnet-class.html#train_momentum" class="summary-sig-name"><code>train_momentum</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>input</span>,
          <span class=summary-sig-arg>target</span>,
          <span class=summary-sig-arg>eta</span>,
          <span class=summary-sig-arg>momentum</span>,
          <span class=summary-sig-arg>maxiter</span>,
          <span class=summary-sig-arg>disp</span>)</span></code>
<br />
Simple backpropagation training with momentum.</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="ffnet.ffnet.ffnet-class.html#train_rprop" class="summary-sig-name"><code>train_rprop</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>input</span>,
          <span class=summary-sig-arg>target</span>,
          <span class=summary-sig-arg>a</span>,
          <span class=summary-sig-arg>b</span>,
          <span class=summary-sig-arg>mimin</span>,
          <span class=summary-sig-arg>mimax</span>,
          <span class=summary-sig-arg>xmi</span>,
          <span class=summary-sig-arg>maxiter</span>,
          <span class=summary-sig-arg>disp</span>)</span></code>
<br />
Rprop training algorithm.</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="ffnet.ffnet.ffnet-class.html#train_tnc" class="summary-sig-name"><code>train_tnc</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>input</span>,
          <span class=summary-sig-arg>target</span>,
          <span class="summary-sig-kwarg">**kwargs</span>)</span></code>
<br />
Train network with a TNC algorithm.</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="../private/ffnet.ffnet.ffnet-class.html#_setnorm" class="summary-sig-name"><code>_setnorm</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>input</span>,
          <span class=summary-sig-arg>target</span>)</span></code>
<br />
Retrieves normalization info from training data and normalizes 
data</td></tr>
<tr><td align="right" valign="top" width="15%"><font size="-1">&nbsp;</font></td>
  <td><code><span class="summary-sig"><a href="../private/ffnet.ffnet.ffnet-class.html#_testdata" class="summary-sig-name"><code>_testdata</code></a>(<span class=summary-sig-arg>self</span>,
          <span class=summary-sig-arg>input</span>,
          <span class=summary-sig-arg>target</span>)</span></code>
<br />
Tests input and target data</td></tr>
</table><br />


<!-- =========== START OF METHOD DETAILS =========== -->
<table class="details" border="1" cellpadding="3" cellspacing="0" width="100%" bgcolor="white">
<tr bgcolor="#70b0f0" class="details">
  <th colspan="2">Method Details</th></tr>
</table>

<a name="call"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">call</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>inp</span>)</span>
  </h3>
  Returns network answer to input sequence
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="derivative"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">derivative</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>inp</span>)</span>
  </h3>
<pre class="literalblock">
Returns partial derivatives of the network's 
output vs its input at given input point
in the following array:
    | o1/i1, o1/i2, ..., o1/in |
    | o2/i1, o2/i2, ..., o2/in |
    | ...                      |
    | om/i1, om/i2, ..., om/in |
</pre>
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="randomweights"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">randomweights</span>(<span class=sig-arg>self</span>)</span>
  </h3>
  Randomize weights due to Bottou proposition
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="sqerror"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">sqerror</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>input</span>,
          <span class=sig-arg>target</span>)</span>
  </h3>
  Returns 0.5*(sum of squared errors at output) for input and target 
  arrays being first normalized. Might be slow in frequent use, because 
  data normalization is performed at ach call. (_setnorm should be called 
  before sqerror - will be changed in future)
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="sqgrad"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">sqgrad</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>input</span>,
          <span class=sig-arg>target</span>)</span>
  </h3>
  Returns gradient of sqerror vs. network weights. Input and target 
  arrays are first normalized. Might be slow in frequent use, because 
  data normalization is performed at each call. (_setnorm should be 
  called before sqgrad - will be changed in future)
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="test"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">test</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>input</span>,
          <span class=sig-arg>target</span>,
          <span class=sig-arg>iprint</span>=<span class=sig-default>1</span>,
          <span class=sig-arg>filename</span>=<span class=sig-default>None</span>)</span>
  </h3>
<pre class="literalblock">
Calculates output and parameters of regression line of targets vs. outputs.
Returns: (output, regress)
where regress contains regression line parameters for each output node. These
parameters are (slope, intercept, r, two-tailed prob, stderr-of-the-estimate).

Optional parameters:
iprint   - verbosity level:
           0 - print nothing
           1 - print regression parameters for each output node
           2 - print additionaly general network info and all targets vs. outputs
               (default is 1)
filename - string with path to the file; if given, output is redirected 
           to this file (default is None)
</pre>
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="train_bfgs"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">train_bfgs</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>input</span>,
          <span class=sig-arg>target</span>,
          <span class="sig-kwarg">**kwargs</span>)</span>
  </h3>
<pre class="literalblock">
Train network with constrained version of quasi-Newton method
of Broyden, Fletcher, Goldfarb, and Shanno (BFGS). Algorithm is
called L_BFGS_B.

Allowed parameters:
bounds  -- a list of (min, max) pairs for each weight, defining
           the bounds on that parameter. Use None for one of min or max
           when there is no bound in that direction. At default all weights
           are bounded to (-100, 100)
m       -- the maximum number of variable metric corrections
           used to define the limited memory matrix. (the limited memory BFGS
           method does not store the full hessian but uses this many terms in an
           approximation to it). Default is 10.
factr   -- The iteration stops when
           (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} &lt;= factr*epsmch
           where f is current cost function value and
           epsmch is the machine precision, which is automatically
           generated by the code. Typical values for factr: 1e12 for
           low accuracy; 1e7 for moderate accuracy; 10.0 for extremely
           high accuracy. Default is 1e7.
pgtol   -- The iteration will stop when
           max{|proj g_i | i = 1, ..., n} &lt;= pgtol
           where pg_i is the ith component of the projected gradient.
           Default is 1e-5.
iprint  -- controls the frequency of output. &lt;0 means no output.
           Default is -1.
maxfun  -- maximum number of cost function evaluations. Default is 15000.

Note: optimization routine used here is part of scipy.optimize.
Note: there exist copyright notice for original optimization code:

License of L-BFGS-B (Fortran code)
==================================
The version included here (in fortran code) is 2.1 (released in 1997). It was
written by Ciyou Zhu, Richard Byrd, and Jorge Nocedal &lt;nocedal&#64;ece.nwu.edu&gt;. It
carries the following condition for use:

This software is freely available, but we expect that all publications
describing  work using this software , or all commercial products using it,
quote at least one of the references given below.

References
* R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound
  Constrained Optimization, (1995), SIAM Journal on Scientific and
  Statistical Computing , 16, 5, pp. 1190-1208.
* C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,
   FORTRAN routines for large scale bound constrained optimization (1997),
   ACM Transactions on Mathematical Software, Vol 23, Num. 4, pp. 550 - 560.
</pre>
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="train_cg"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">train_cg</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>input</span>,
          <span class=sig-arg>target</span>,
          <span class="sig-kwarg">**kwargs</span>)</span>
  </h3>
<pre class="literalblock">
Train network with conjugate gradient algorithm using the
nonlinear conjugate gradient algorithm of Polak and Ribiere.
See Wright, and Nocedal 'Numerical Optimization', 1999, pg. 120-122.

Allowed parameters:
gtol         - stop when norm of gradient is less than gtol
               (default is 1e-5)
norm         - order of vector norm to use (default is infinity)
maxiter      - the maximum number of iterations (default is 10000)
disp         - print convergence message at the end of training
               if non-zero (default is 1)

Note: this procedure does not produce any output during training.
Note: optimization routine used here is part of scipy.optimize.
</pre>
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="train_genetic"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">train_genetic</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>input</span>,
          <span class=sig-arg>target</span>,
          <span class="sig-kwarg">**kwargs</span>)</span>
  </h3>
<pre class="literalblock">
Global weights optimization with genetic algorithm.

Allowed parameters:
lower        - lower bound of weights values (default is -25.)
upper        - upper bound of weights values (default is 25.)
individuals  - number of individuals in a population (default
               is 20)
generations  - number of generations over which solution is
               to evolve (default is 500)
crossover    - crossover probability; must be  &lt;= 1.0 (default
               is 0.85). If crossover takes place, either one
               or two splicing points are used, with equal
               probabilities
mutation     - 1/2/3/4/5 (default is 2)
               1=one-point mutation, fixed rate
               2=one-point, adjustable rate based on fitness
               3=one-point, adjustable rate based on distance
               4=one-point+creep, fixed rate
               5=one-point+creep, adjustable rate based on fitness
               6=one-point+creep, adjustable rate based on distance
initrate     - initial mutation rate; should be small (default
               is 0.005) (Note: the mutation rate is the proba-
               bility that any one gene locus will mutate in
               any one generation.)
minrate      - minimum mutation rate; must be &gt;= 0.0 (default
               is 0.0005)
maxrate      - maximum mutation rate; must be &lt;= 1.0 (default
               is 0.25)
fitnessdiff  - relative fitness differential; range from 0
               (none) to 1 (maximum).  (default is 1.)
reproduction - reproduction plan; 1/2/3=Full generational
               replacement/Steady-state-replace-random/Steady-
               state-replace-worst (default is 3)
elitism      - elitism flag; 0/1=off/on (default is 0)
               (Applies only to reproduction plans 1 and 2)
verbosity    - printed output 0/1/2=None/Minimal/Verbose
               (default is 0)

Note: this optimization routine is a python port of fortran pikaia code.

For more info see pikaia homepage and documentation:
http://www.hao.ucar.edu/Public/models/pikaia/pikaia.html
</pre>
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="train_momentum"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">train_momentum</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>input</span>,
          <span class=sig-arg>target</span>,
          <span class=sig-arg>eta</span>=<span class=sig-default>0.20000000000000001</span>,
          <span class=sig-arg>momentum</span>=<span class=sig-default>0.80000000000000004</span>,
          <span class=sig-arg>maxiter</span>=<span class=sig-default>10000</span>,
          <span class=sig-arg>disp</span>=<span class=sig-default>0</span>)</span>
  </h3>
  <p>Simple backpropagation training with momentum.</p>
  Allowed parameters: eta - descent scaling parameter (default is 0.2) 
  momentum - momentum coefficient (default is 0.8) maxiter - the maximum 
  number of iterations (default is 10000) disp - print convergence 
  message if non-zero (default is 0)
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="train_rprop"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">train_rprop</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>input</span>,
          <span class=sig-arg>target</span>,
          <span class=sig-arg>a</span>=<span class=sig-default>1.2</span>,
          <span class=sig-arg>b</span>=<span class=sig-default>0.5</span>,
          <span class=sig-arg>mimin</span>=<span class=sig-default>9.9999999999999995e-07</span>,
          <span class=sig-arg>mimax</span>=<span class=sig-default>50.0</span>,
          <span class=sig-arg>xmi</span>=<span class=sig-default>0.10000000000000001</span>,
          <span class=sig-arg>maxiter</span>=<span class=sig-default>10000</span>,
          <span class=sig-arg>disp</span>=<span class=sig-default>0</span>)</span>
  </h3>
<pre class="literalblock">
Rprop training algorithm.

Allowed parameters:
a               - training step increasing parameter (default is 1.2)
b               - training step decreasing parameter (default is 0.5)
mimin           - minimum training step (default is 0.000001)
mimax           - maximum training step (default is 50.)
xmi             - vector containing initial training steps for weights;
                  if 'xmi' is a scalar then its value is set for all
                  weights (default is 0.1)
maxiter         - the maximum number of iterations (default is 10000)
disp            - print convergence message if non-zero (default is 0)

Method updates network weights and returns 'xmi' vector 
(after 'maxiter' iterations).
</pre>
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="train_tnc"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">train_tnc</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>input</span>,
          <span class=sig-arg>target</span>,
          <span class="sig-kwarg">**kwargs</span>)</span>
  </h3>
<pre class="literalblock">
Train network with a TNC algorithm.
TNC is a C implementation of TNBC, a truncated newton
optimization package originally developed by Stephen G. Nash in Fortran.

Allowed parameters:
bounds    : a list of (min, max) pairs for each weight, defining
            the bounds on that parameter. Use None for one of min or max
            when there is no bound in that direction. At default all weights
            are bounded to (-100, 100)
scale      : scaling factors to apply to each weight (a list of floats)
            if None, the factors are up-low for interval bounded weights
            and 1+|weight| for the others.
            defaults to None
messages  : bit mask used to select messages display during minimization
            0: 'No messages',
            1: 'One line per iteration',
            2: 'Informational messages',
            4: 'Version info',
            8: 'Exit reasons',
            15: 'All messages'
            defaults to 0.
maxCGit   : max. number of hessian*vector evaluation per main iteration
            if maxCGit == 0, the direction chosen is -gradient
            if maxCGit &lt; 0, maxCGit is set to max(1,min(50,n/2))
            defaults to -1
maxfun    : max. number of function evaluation
            if None, maxnfeval is set to max(1000, 100*len(x0))
            defaults to None
eta       : severity of the line search. if &lt; 0 or &gt; 1, set to 0.25
            defaults to -1
stepmx    : maximum step for the line search. may be increased during call
            if too small, will be set to 10.0
            defaults to 0
accuracy  : relative precision for finite difference calculations
            if &lt;= machine_precision, set to sqrt(machine_precision)
            defaults to 0
fmin      : minimum cost function value estimate
            defaults to 0
ftol      : precision goal for the value of f in the stopping criterion
            relative to the machine precision and the value of f.
            if ftol &lt; 0.0, ftol is set to 0.0
            defaults to 0
rescale   : Scaling factor (in log10) used to trigger rescaling
            if 0, rescale at each iteration
            if a large value, never rescale
            if &lt; 0, rescale is set to 1.3.
            default to -1

Note: optimization routine used here is part of scipy.optimize.
</pre>
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="_setnorm"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">_setnorm</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>input</span>=<span class=sig-default>None</span>,
          <span class=sig-arg>target</span>=<span class=sig-default>None</span>)</span>
  </h3>
  Retrieves normalization info from training data and normalizes 
  data
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>

<a name="_testdata"></a>
<table width="100%" class="func-details" bgcolor="#e0e0e0"><tr><td>
  <h3><span class="sig"><span class="sig-name">_testdata</span>(<span class=sig-arg>self</span>,
          <span class=sig-arg>input</span>,
          <span class=sig-arg>target</span>)</span>
  </h3>
  Tests input and target data
  <dl><dt></dt><dd>
  </dd></dl>
</td></tr></table>
<br />


<!-- =========== START OF NAVBAR =========== -->
<table class="navbar" border="0" width="100%" cellpadding="0" bgcolor="#a0c0ff" cellspacing="0">
  <tr valign="center">
    <th class="navbar">&nbsp;&nbsp;&nbsp;<a class="navbar" href="ffnet-module.html">Home</a>&nbsp;&nbsp;&nbsp;</th>
    <th class="navbar">&nbsp;&nbsp;&nbsp;<a class="navbar" href="trees.html">Trees</a>&nbsp;&nbsp;&nbsp;</th>
    <th class="navbar">&nbsp;&nbsp;&nbsp;<a class="navbar" href="indices.html">Index</a>&nbsp;&nbsp;&nbsp;</th>
    <th class="navbar">&nbsp;&nbsp;&nbsp;<a class="navbar" href="help.html">Help</a>&nbsp;&nbsp;&nbsp;</th>
    <th class="navbar" width="100%"></th>
  </tr>
</table>

<table border="0" cellpadding="0" cellspacing="0" width="100%">
  <tr>
    <td align="left"><font size="-2">Generated by Epydoc 2.1 on Thu Mar 22 01:50:57 2007</font></td>
    <td align="right"><a href="http://epydoc.sourceforge.net"
                      ><font size="-2">http://epydoc.sf.net</font></a></td>
  </tr>
</table>
</body>
</html>
